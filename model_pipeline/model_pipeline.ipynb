{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf3956c0",
   "metadata": {},
   "source": [
    "# Getting started:\n",
    "\n",
    "Create a virtual environment to manage dependencies and avoid conflicts with other projects. You can do this using `venv` or `conda`.\n",
    "\n",
    "Then, ensure that the necessary basic libraries are installed. You can install them using the following command:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "The script also quires the following:\n",
    "- Pytorch: https://pytorch.org/get-started/locally/\n",
    "- Segment Anything Model (SAM):\n",
    "    ```bash\n",
    "    pip install git+https://github.com/facebookresearch/segment-anything.git\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78055e71",
   "metadata": {},
   "source": [
    "# Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52122e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a41ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"image.png\")\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c61629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "import os, urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f0939",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 10\n",
    "\n",
    "height, width = image.size[1], image.size[0]\n",
    "print(f\"Image size: {width}x{height}\")\n",
    "\n",
    "checkpoint_path = \"sam_vit_b_01ec64.pth\"\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\"\n",
    "    urllib.request.urlretrieve(url, checkpoint_path)\n",
    "\n",
    "\n",
    "sam = sam_model_registry[\"vit_b\"](checkpoint=checkpoint_path)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sam.to(device)\n",
    "print(f\"SAM loaded on {device}\")\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    points_per_side=16,\n",
    "    pred_iou_thresh=0.7,\n",
    "    stability_score_thresh=0.85,\n",
    "    min_mask_region_area=500,\n",
    ")\n",
    "\n",
    "print(\"Generating masks...\")\n",
    "masks = mask_generator.generate(np.array(image))\n",
    "masks_sorted = sorted(masks, key=lambda x: x['area'], reverse=True)\n",
    "\n",
    "predicted_panoptic_map = np.zeros((height, width), dtype=np.int32)\n",
    "for idx, mask_data in enumerate(masks_sorted[:TOP_N]):\n",
    "    predicted_panoptic_map[mask_data['segmentation']] = idx + 1\n",
    "\n",
    "predicted_panoptic_map = torch.from_numpy(predicted_panoptic_map)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(predicted_panoptic_map, cmap=f'tab20', interpolation='nearest')\n",
    "plt.title(f'SAM Segmentation ({len(masks_sorted[:TOP_N])} segments)')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d7a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as tf\n",
    "import torch\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tf.Compose([tf.PILToTensor()])\n",
    "img_tensor = transform(image)\n",
    "\n",
    "def get_mask_box(tensor: torch.Tensor) -> tuple:\n",
    "    \"\"\"\n",
    "    Get part of the bounding box of the non-zero elements in a tensor.\n",
    "    Args:\n",
    "        tensor (torch.Tensor): Input tensor.\n",
    "    Returns:\n",
    "        tuple: (first_n, last_n) where first_n is the index of the first non-zero element,\n",
    "               and last_n is the index of the last non-zero element.\n",
    "    \"\"\"\n",
    "\n",
    "    non_zero_indices = torch.nonzero(tensor, as_tuple=True)[0]\n",
    "    if non_zero_indices.shape[0] == 0:\n",
    "        return None, None\n",
    "    first_n = non_zero_indices[:1].item()\n",
    "    last_n = non_zero_indices[-1:].item()\n",
    "\n",
    "    return first_n, last_n\n",
    "\n",
    "segments = []\n",
    "for label in predicted_panoptic_map.unique():\n",
    "    \n",
    "    y_start, y_end = get_mask_box(predicted_panoptic_map==label)\n",
    "    x_start, x_end = get_mask_box((predicted_panoptic_map==label).T)\n",
    "\n",
    "    cropped_tensor = img_tensor[:, y_start:y_end+1, x_start:x_end+1]\n",
    "    cropped_mask = predicted_panoptic_map[y_start:y_end+1, x_start:x_end+1] == label\n",
    "\n",
    "    segment = cropped_tensor * cropped_mask.unsqueeze(0)\n",
    "    segment[:, ~cropped_mask] = 188\n",
    "\n",
    "    segments.append(segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d28dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\n",
    "class_model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6016f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = []\n",
    "\n",
    "for segment in segments:\n",
    "    inputs = image_processor(images=segment, return_tensors=\"pt\")\n",
    "    outputs = class_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_idx = logits.argmax(-1).item()\n",
    "    predicted_class = class_model.config.id2label[predicted_class_idx]\n",
    "    predicted_classes.append(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350639d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_classifier = pipeline(\"zero-shot-classification\", model=\"typeform/distilbert-base-uncased-mnli\")\n",
    "candidate_labels = [\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"tree\",\n",
    "    \"dog\",\n",
    "    \"building\",\n",
    "    \"person\",\n",
    "    \"sky\",\n",
    "    \"ground\",\n",
    "    \"hardware\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5481ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "for predicted_class in predicted_classes:\n",
    "    result = label_classifier(predicted_class, candidate_labels=candidate_labels)\n",
    "    label = result['labels'][0] \n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af117ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for segment, label, predicted_class in zip(segments, labels, predicted_classes):\n",
    "    plt.imshow(segment.permute(1, 2, 0).numpy().astype(int))\n",
    "    plt.title(f\"Predicted Class: {predicted_class}, Label: {label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
